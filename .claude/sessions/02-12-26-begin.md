# Session: 2026-02-12 — Phase 1 Begin (v0.4) — CLOSED

## Context

Phase 1 of the fractal-plan roadmap says:

> - Rust workspace with core library crate
> - DuckDB + LanceDB integration with shared Arrow buffers
> - Basic ESH data schema (Arrow schema definitions)
> - Data ingestion pipeline (regulatory documents -> embeddings -> LanceDB)
> - CLI interface for queries

The workspace skeleton is done (6 crates, WIT interfaces, CI, git hooks). Two placeholder Arrow schemas exist in `fractalaw-core` (site_compliance, audit_log). These are generic stubs. The real data model hasn't been designed yet.

## The Gap: Legacy → Fractalaw

The existing system ([sertantai-legal](https://github.com/shotleybuilder/sertantai-legal)) is a Phoenix/Ash app backed by PostgreSQL with ~19K UK legislation records. Its `uk_lrt` table is a wide denormalized table (~80+ columns) with:

**What it has:**
- 6-stage scraping pipeline parsing legislation.gov.uk XML (metadata → extent → enacted_by → amendments → repeal/revoke → taxa/DRRP classification)
- Self-referencing graph relationships: `enacted_by`, `amending`, `amended_by`, `rescinding`, `rescinded_by` — stored as `text[]` arrays of law name keys, plus `linked_*` variants resolved to existing records
- Rich JSONB `*_stats_per_law` structures capturing amendment detail down to individual article targets and affect types
- DRRP holder/taxonomy classification (Duties, Rights, Responsibilities, Powers)
- Geographic extent (E+W+S+NI regions)
- Live status reconciliation from two data sources (changes endpoint vs metadata endpoint, "most severe wins")
- AI via external services (RunPod) — brute force, not local

**What Fractalaw changes:**
- PostgreSQL → DuckDB (analytical) + LanceDB (vector/semantic)
- Row-oriented → Columnar (Arrow)
- Hosted/browser-served → Local-first, edge-deployed
- Cloud AI (RunPod) → Local quantised inference (ONNX, MLC LLM)
- Single server → Hub + edge nodes with sync
- Monolith → WASM micro-apps

This is not a port. It's a re-architecture. The question is: what's the right **data model** for columnar, local-first, graph-aware legislation data?

**Scope: Multi-jurisdiction, multi-language.** The legacy app is UK-only (`uk_lrt`). Fractalaw is designed from the outset for Europe-wide and Australia/New Zealand coverage, with full language support. The schema must not be UK-specific — `type_code` values like `ukpga`, `uksi`, `nisro` are UK legislation.gov.uk concepts. The Fractalaw schema needs a jurisdiction-aware design where UK is the first dataset but the structure accommodates EU directives, French codes, German Verordnungen, Australian Acts, NZ regulations, etc. The ultimate master data source varies per jurisdiction (legislation.gov.uk for UK, EUR-Lex for EU, etc.).

---

## Architecture Decision: Graph-Dense Columnar (v0.2)

**Decision: Hybrid "graph-dense columnar" with three access tiers.**

The guiding constraint is **no pointer chasing** — the CPU should not jump to a different memory address to answer the common case. The fractal-plan's Locality of Reference and Locality of Logic principles map directly to how we partition the data:

### Three Access Tiers

```
┌─────────────────────────────────────────────────────────────┐
│                     HOT PATH (same row)                     │
│              DuckDB: legislation table                       │
│                                                             │
│  Metadata + List<Struct> relationship arrays                │
│  One row = one law + its immediate context                  │
│  Spatial locality: CPU reads one memory block               │
│                                                             │
│  amended_by: [{name, title, year, count, latest_date}, ...] │
│  enacted_by: [{name, title, year}, ...]                     │
│  rescinding: [{name, title, year, count}, ...]              │
├─────────────────────────────────────────────────────────────┤
│                  ANALYTICAL PATH (edge table)               │
│              DuckDB: law_edges table                         │
│                                                             │
│  Flattened edge table for vectorised joins                  │
│  Complex traversals, multi-hop graph queries                │
│  Fan-in/fan-out analytics, amendment network analysis       │
│                                                             │
│  (source_name, target_name, edge_type, article_target,      │
│   affect_type, applied_status, date)                        │
├─────────────────────────────────────────────────────────────┤
│                  SEMANTIC PATH (vector store)               │
│              LanceDB: legislation_text table                 │
│                                                             │
│  Full text, chunked sections, embeddings                    │
│  Semantic search, RAG retrieval                             │
│  AI classifications, extracted obligations                   │
└─────────────────────────────────────────────────────────────┘
```

### Why This Works

**Hot path — `List<Struct>` on the legislation row:**
- The `amended_by` column isn't just `List<Utf8>` (a list of IDs). It's `List<Struct>` carrying **denormalized immediate context**: name, title, year, amendment count, latest date.
- This is **Locality of Logic** — the row contains not just what it references, but enough about the referenced law to answer the common question without a second lookup.
- "Show me Law X and what amended it" = one row read. No join. No pointer chase.
- The legacy `linked_amended_by` + `*_stats_per_law` JSONB are effectively this pattern already — we're making it explicit in Arrow's type system.

**Analytical path — flattened `law_edges` table:**
- For complex queries ("find all laws within 2 hops of the Environmental Protection Act") the `List<Struct>` columns would require `UNNEST` + self-join, which loses the vectorisation benefit.
- A dedicated flattened edge table with `(source, target, type, detail...)` enables DuckDB's morsel-driven parallel joins and columnar scans.
- This is the **search index** — derived from the same data, optimised for a different access pattern.
- The edge table is a materialised view of the relationship data. It can be rebuilt from the legislation table's `List<Struct>` columns.

**Semantic path — LanceDB:**
- Legal text, section-level chunks, embedding vectors.
- DataFusion bridges DuckDB metadata queries with LanceDB semantic search in a single SQL plan.

### Denormalized Immediate Context

The key insight: relationship arrays carry **structs, not just IDs**.

Legacy PostgreSQL:
```
amended_by: ["UK_uksi_2023_381", "UK_uksi_2020_240"]  -- just IDs, need JOIN for title
```

Fractalaw Arrow:
```
amended_by: [
  {name: "UK_uksi_2023_381", title: "The Health and Care...", year: 2023, count: 4, latest_date: 2023-10-15},
  {name: "UK_uksi_2020_240", title: "The Statutory Parental...", year: 2020, count: 1, latest_date: 2020-04-06}
]
```

This is the same principle as the legacy `*_stats_per_law` JSONB — but promoted to a first-class Arrow `List<Struct>` type with named fields, type safety, and columnar encoding. The CPU doesn't need to chase a pointer to another table to display "amended by" in a UI or compliance report.

### Consistency Between Tiers

The hot path and analytical path contain the **same data in different shapes**:
- `legislation.amended_by` (List<Struct>) = the embedded view
- `law_edges WHERE edge_type = 'amended_by'` = the flattened view

On write, both are updated. On read, the query planner (DataFusion) picks the optimal path:
- Single-law lookup → hot path (row read)
- Multi-hop traversal → analytical path (vectorised join)

---

## What Goes Where

| Data | Store | Access Pattern |
|------|-------|---------------|
| Legislation metadata (identity, dates, status, extent, domain, family) | DuckDB: `legislation` (LRT) | Hot path — filter, aggregate, single-law lookup |
| Relationship arrays with immediate context (enacted_by, amending, amended_by, rescinding, rescinded_by) | DuckDB: `legislation` as `List<Struct>` | Hot path — display relationships without join |
| DRRP taxa (duties, rights, responsibilities, powers) | DuckDB: `legislation` as `List<Struct>` | Hot path — compliance matching, categorical filtering |
| Flattened relationship edges with article-level detail | DuckDB: `law_edges` | Analytical path — graph traversal, vectorised joins, amendment network analysis |
| Legislation full text (body, schedules, section chunks) | LanceDB: `legislation_text` (LAT) | Semantic path — embedding source, full-text search |
| Amendment/modification/commencement annotations (F/C/I/E codes) | LanceDB: `amendment_annotations` | Semantic path — annotation detail linked to LAT sections |
| Embedding vectors (per-section) | LanceDB: `legislation_text` | Semantic path — vector similarity, RAG retrieval |
| AI classifications & extracted obligations | LanceDB: `legislation_text` (metadata columns) | Semantic + structured hybrid queries |

---

## The Scraping Pipeline

The existing 6-stage parser in Elixir hits legislation.gov.uk endpoints and builds up each record progressively. This pipeline needs to exist in Fractalaw — it's how data gets in. But now it's a WASM micro-app (`regulation-importer`) that writes through the `fractal:data/mutate` WIT interface.

Not yet though. Phase 1 = get data in from the existing PostgreSQL export. Phase 3 = build the scraper as a micro-app.

---

## Phase 1 Tasks

### 1. Design the Arrow schemas
- [x] **`legislation` schema (LRT)** — 74 columns across 12 sections (identity, classification, dates, territorial extent, document stats, status, function, relationships, DRRP taxa, change logs, timestamps, dropped columns)
- [x] **`law_edges` schema** — 8 columns (source_name, target_name, edge_type, jurisdiction, article_target, affect_type, applied_status, date)
- [x] **`legislation_text` schema (LAT)** — 27 columns (identity/position, structural hierarchy, section types, content, amendment annotation counts, embeddings placeholder, metadata)
- [x] **`amendment_annotations` schema** — 8 columns (identity, annotation codes F/C/I/E, content with affected_sections linkage, metadata)
- [x] Decide which derived columns are stored vs computed at query time (date year/month → computed; amendment counts → stored)
- [x] Document ordering problem identified and resolved (`position` Int32 for document-traversal order, `section_id` as human-readable address)
- [x] Multi-jurisdiction territorial extent: two-layer model (sub-national regions + national)
- [x] `subjects` column: retained with analysis ([md-subjects-analysis](https://github.com/shotleybuilder/sertantai-legal/blob/main/docs/md-subjects-analysis.md))
- [x] Schema documented in [`docs/SCHEMA.md`](../../docs/SCHEMA.md) — 4 tables, struct definitions, column counts, migration path
- [x] Implement schemas in `fractalaw-core/src/schema.rs` — 5 public fns (`legislation_schema` 78 cols, `law_edges_schema` 8 cols, `legislation_text_schema` 27 cols, `amendment_annotations_schema` 8 cols, `audit_log_schema` 10 cols), 3 private helpers (`related_law_struct`, `drrp_entry_struct`, `timestamp_ns_utc`)
- [x] Unit tests: 16 tests — field counts, nullability, `List<Struct>` shape (RelatedLaw 5 fields, DRRPEntry 4 fields), `FixedSizeList<Float32, 384>` embedding, `Timestamp(ns, UTC)`

### 2. Export legacy data to Parquet
- [x] Export `uk_lrt` from dev PostgreSQL to JSONL (19,318 rows, 309MB) — transferred from laptop via LAN
- [x] Write DuckDB SQL transform script ([`data/export_legislation.sql`](../../data/export_legislation.sql)) — replaced Elixir mix task approach with DuckDB CLI (`brew install duckdb`)
- [x] Map LRT columns per SCHEMA.md (set `jurisdiction = "UK"`, `source_authority = "legislation.gov.uk"`, `language = "en"`)
- [x] Flatten `*_stats_per_law` MAP(VARCHAR, STRUCT) into `RelatedLaw` struct arrays via `LATERAL unnest(map_entries(...))`
- [x] Derive `law_edges` Parquet from relationship columns + `*_stats_per_law` detail (double LATERAL unnest for article-level edges)
- [x] Validate record counts: 19,318 LRT rows (78 cols, 12MB), 1,035,305 edges (8 cols, 7.1MB)
- [x] **[PARKED]** Export LAT content from Airtable CSV exports (17 files, 460 laws) — [`data/export_lat.sql`](../../data/export_lat.sql) transforms LAT-*.csv → `legislation_text.parquet` (99,113 rows, 27 cols, 6.8MB). Completed but critical review ([`docs/SCHEMA-2.0.md`](../../docs/SCHEMA-2.0.md)) found `section_id` has 1.5% collision rate, annotation IDs have 2.8% collision rate, and the schema doesn't generalise to non-UK jurisdictions. LAT data cleanup will happen in the sister project (server-client architecture) first, providing a clean baseline for Fractalaw.
- [x] **[PARKED]** Export `amendment_annotations` — same issues as LAT. 21,929 rows with F/C/I/E codes from LAT + AMD sources. Parked pending sister project cleanup.
- [x] **[PARKED]** Export `annotation_totals.parquet` — 136 laws. Parked.
- [x] Non-UK LAT files surveyed (7 countries: AUT, DK, FIN, DE, NO, SWE, TUR) — confirmed incompatible column schemas, need per-jurisdiction reparsing. Renamed to `xLAT-*.csv` to exclude from glob. Deferred.

### 3. Build DuckDB ingestion in fractalaw-store
Plan: [`.claude/sessions/02-17-26-duckdb-ingestion-plan.md`](02-17-26-duckdb-ingestion-plan.md)
- [x] Implement `fractalaw-store` with DuckDB feature enabled — `DuckStore` struct in `crates/fractalaw-store/src/duck.rs`, `StoreError` in `error.rs`, feature-gated module in `lib.rs` (~280 lines across 3 files)
- [x] Load Parquet into DuckDB `legislation` table — `DuckStore::load_legislation()` via `read_parquet()`, 19,318 rows / 78 cols verified
- [x] Load flattened edges Parquet into `law_edges` table — `DuckStore::load_law_edges()`, 1,035,305 rows / 8 cols verified
- [x] Verify: `SELECT count(*) FROM legislation` returns ~19K — `legislation_count()` test passes (19,318)
- [x] Verify: hot path query — `get_legislation("UK_ukpga_1974_37")` returns 1 row, 78 cols including `List<Struct>` relationship arrays
- [x] Verify: analytical path — `laws_within_hops("UK_ukpga_1974_37", 2)` recursive CTE returns reachable laws with hop distance
- [x] 11 tests, all passing (32s with real data). Additional API: `query_legislation_sql()`, `edges_for_law()`, `query_arrow()`, `connection()`
- **Note:** Arrow version split — duckdb 1.4.4 bundles arrow 56, workspace uses arrow 57. `duck.rs` uses `duckdb::arrow` re-exports internally. Build requires `CXX=$(which g++) LIBRARY_PATH=".../gcc/15"` on Fedora Bluefin. Resolution planned in [02-19-26-arrow-versioning.md](02-19-26-arrow-versioning.md) — pin duckdb to git main which already has arrow 57.

### 4. Build LanceDB text store in fractalaw-store — **[PARKED]**
- [ ] Create LanceDB `legislation_text` table from LAT schema
- [ ] Create LanceDB `amendment_annotations` table
- [ ] Load text content from export (article-level text with position ordering)
- [ ] Load amendment annotations with `affected_sections` linkage to LAT
- [ ] Embedding vector column present in schema but null — populated in later phase when ONNX is integrated
- [ ] Verify: full-text search works on loaded text (LanceDB native FTS)
- [ ] Verify: metadata filtering works (by law_name, section_type, language)
- [ ] Verify: `ORDER BY position` recovers published document order
Blocked on LAT schema cleanup. The SCHEMA-2.0 review identified fundamental issues with the Airtable-originated `section_id` encoding and annotation ID uniqueness. LAT data will be cleaned in the sister project first, then re-ingested here with corrected IDs. LanceDB integration (text store, embeddings, FTS) deferred until clean LAT data is available.

### 5. Wire up DataFusion as unified query layer
Plan: [`.claude/sessions/02-19-26-plan-wire-up-datafusion.md`](02-19-26-plan-wire-up-datafusion.md)
- [x] Register DuckDB `TableProvider` for both `legislation` and `law_edges` — custom `DuckTableProvider` with projection + limit pushdown, `Mutex<Connection>` for `Send + Sync`
- [PARKED] Register LanceDB `TableProvider` for `legislation_text` and `amendment_annotations`
- [PARKED] Verify: single SQL query can span DuckDB metadata + LanceDB text search
- [x] Verify: DataFusion can query both tables — `FusionStore` in `crates/fractalaw-store/src/fusion.rs`, count queries, cross-table JOIN, WHERE filters all verified
- [x] Basic UDFs: `law_status()`, `edge_type_label()` — both registered as `Volatility::Immutable` scalar UDFs with array + scalar handling
- [x] 10 DataFusion tests passing (register_tables, count_legislation, count_law_edges, projection_pushdown, where_filter, limit_pushdown, cross_table_join, udf_law_status, udf_edge_type_label, udf_passthrough) — 49s with real data
- [x] `.cargo/config.toml` created with `CC`/`CXX` (ccache + brew gcc) and `LIBRARY_PATH` — eliminates manual env vars on every build command
- [x] Build skill documented in [`.claude/skills/native-compilation/SKILL.md`](../skills/native-compilation/SKILL.md) — compilation lessons learned for future sessions

### 6. Build CLI query interface
Plan: [`.claude/plans/mighty-mapping-star.md`](../../.claude/plans/mighty-mapping-star.md)
- [x] Extend `fractalaw-cli` with commands:
  - `fractalaw query "SELECT ..."` — SQL via DataFusion (async, UDFs available)
  - `fractalaw law <name>` — vertical card display with denormalized relationships (hot path) + edges table
  - `fractalaw graph <name> [--hops N]` — recursive graph traversal (analytical path, default 2 hops)
  - [PARKED] `fractalaw text <name>` — show article text in published order with amendment annotations
  - `fractalaw stats` — dataset summary (row counts, year range, status/edge_type/jurisdiction breakdowns)
- [x] Arrow pretty-print for tabular output — `arrow::util::pretty::print_batches()` with `prettyprint` feature
- [x] Vertical card formatting in `display.rs` — 13 section groupings, type-dispatch for Utf8/Int32/Date32/Boolean/Timestamp/`List<Utf8>`/`List<Struct>`, null skipping, `MAX_LIST_ITEMS = 10` truncation
- [x] Clap derive for CLI arg parsing — `--data-dir` global flag (default `./data`), binary name `fractalaw` via `[[bin]]`
- [x] DuckDB + DataFusion features enabled unconditionally — aligns with fractal-plan §2.3 (every node runs full engine stack)
- [x] Smoke tested all 4 commands against real data (19,318 legislation rows, 1,035,305 edges)

### 7. Validate the architecture with real queries
- [x] "Show me all statutory instruments from 2024 affecting Scotland that are still in force" (metadata filter) — via `fractalaw query`. Original query returned no results: ESH dataset has no 2024 UKSIs with `extent_code LIKE '%S%'` (most UKSIs apply UK-wide). Adapted to SSI type (`type_code = 'ssi'`) — returns 20+ Scottish instruments from 2020-2026. Query logic confirmed correct; data coverage is the constraint.
- [x] "What laws does the Health and Safety at Work Act 1974 enact?" (hot path) — via `fractalaw law UK_ukpga_1974_37`. One row read, 78 columns, full `List<Struct>` expansion: 1 enacting law, 177 amending laws, 20 rescinding laws. All rendered with name, title, year, amendment count. Zero joins, zero pointer chasing.
- [x] "Show the full amendment network for COSHH Regulations within 2 hops" (analytical path) — via `fractalaw graph UK_uksi_2002_2677 --hops 2`. Recursive CTE on `law_edges` returns **1,942 laws** with hop distance. 23 at hop 1, 1,919 at hop 2.
- [x] "Which laws have amendments not yet applied?" (analytical path) — via `fractalaw query`. **185,511 edges** with `applied_status = 'Not yet'`. Top offender: `UK_uksi_1986_1925` with 2,278 unapplied amendments. Top-10 breakdown retrieved.
- [PARKED] "Show article text for COSHH reg.7 with all amendment annotations" (semantic path: LAT joined with amendment_annotations)
- [PARKED] "Find regulations semantically similar to 'workplace noise exposure limits'" (semantic path: LanceDB vector search — requires Phase 2 embeddings)
- [x] Measure: query latency — ~4s wall time per CLI invocation, dominated by Parquet cold-load (~2.4s per table). Actual query execution sub-ms (hot path) to ~10ms (analytical). In a persistent process, queries near-instant. PostgreSQL baseline comparison deferred — no direct access to legacy DB from this machine.

---

## Open Questions (Updated)

1. **Export format**: ~~Parquet vs Arrow IPC~~ → **Parquet**. DuckDB reads it natively with `read_parquet()`, it's self-describing, and it compresses well for the one-time transfer. **Resolved.**

2. **Embeddings**: ~~Bootstrap now or defer?~~ → **Schema design is Phase 1, embeddings are a later phase.** The text table (LAT — Legal Article Text) schema needs to be designed now alongside the legislation table. A prototype exists as an Airtable concept with real data. The LanceDB `legislation_text` table structure — law reference, section chunking, text fields, metadata — is Phase 1 work. Embedding vector columns are defined in the schema but populated later when ONNX inference is integrated. **Resolved.**

3. **Graph traversal performance**: ~~Edge table vs arrays~~ → **Both**. `List<Struct>` for the hot path (single-row context), flattened edge table for analytical traversals. Benchmark both on the real ~19K dataset to confirm the crossover point. **Resolved by architecture decision.**

4. **Legislation full text**: The legacy app stores `md_description` and article-level text from taxa parsing, but not the full legislation body text. Phase 1 can seed LanceDB with what exists. A future legislation.gov.uk full-text scrape (or the `regulation-importer` micro-app in Phase 3) extends this.

5. **Incremental updates**: Phase 1 is a static snapshot. The scraper micro-app (Phase 3) handles ongoing updates. Both tiers (legislation row + edge table) must be updated atomically — this shapes the write API in `fractal:data/mutate`.

6. **Edge table derivation**: ~~Source of truth question~~ → **Legislation table is source of truth.** This is the scraping target — the record is built up progressively through parse stages. The edge table is derived/materialised from the `List<Struct>` relationship columns. Rebuilt on import. Ultimately, the master data is the jurisdiction's official source (legislation.gov.uk for UK, EUR-Lex for EU, etc.) — the legislation table is Fractalaw's local copy of that truth. **Resolved.**

7. **Multi-jurisdiction schema generalisation**: The legacy `uk_lrt` schema has UK-specific fields (`type_code` = `ukpga`/`uksi`/`nisro`, `leg_gov_uk_url`, SI codes, etc.). The Fractalaw schema must generalise:
   - `jurisdiction` field (UK, EU, FR, DE, AU, NZ, ...)
   - `source_authority` (legislation.gov.uk, EUR-Lex, LEGIFRANCE, ...)
   - `instrument_type` that abstracts across legal traditions (Act/SI/Regulation/Directive/Verordnung/Code)
   - `language` (en, fr, de, ...) with multi-language title/text support
   - `source_url` replacing `leg_gov_uk_url`
   - UK-specific fields (SI codes, old_style_number) as nullable or in a jurisdiction-specific extension struct
   - Geographic extent generalised beyond E+W+S+NI to any sub-national region

## Legacy Reference: legl Project (Airtable Prototype)

The [legl project](https://github.com/shotleybuilder/legl) is an earlier Elixir prototype that scraped legislation into Airtable. It provides two critical reference points:

### Multi-jurisdiction source URLs

Each country has its own canonical legislation source, URL structure, and article naming conventions:

| Country | Source | URL Example |
|---------|--------|-------------|
| UK | legislation.gov.uk | `https://www.legislation.gov.uk/uksi/2024/123` |
| DE | retsinformation.dk (note: module says DK but is DE) | `https://www.retsinformation.dk` |
| NO | lovdata.no | `https://lovdata.no/dokument/SF/forskrift/` |
| FIN | finlex.fi | `https://www.finlex.fi` |
| AUT | finlex.fi (Austrian variant) | — |
| SWE | rkrattsbaser.gov.se | `http://rkrattsbaser.gov.se/` |
| TUR | mevzuat.gov.tr | `https://www.mevzuat.gov.tr` |
| RUS | pravo.gov.ru | `https://www.pravo.gov.ru` |
| DK | retsinformation.dk | `https://www.retsinformation.dk` |

**Key observation:** Every jurisdiction has different article-level terminology. The `AirtableSchema` type defines this per-country:
- UK: part → chapter → section → article → paragraph → sub-paragraph, with schedules
- DE/DK: part → chapter → section → sub_section → article → para → sub
- TUR: kisim → bölüm → madde → alt-makale, with ek (annex) and geçici-madde (amendment)
- RUS: chast → razdel → glava → stat'ya → abzats → podpunkt
- NO: kapittel → paragraf → ledd

The Fractalaw LAT schema must accommodate this — **`section_type` needs to be a flexible enum, not hardcoded to UK terminology**. The hierarchy depth varies (UK Acts have sections, UK SIs have articles/regulations).

### LAT Record Shape (from legl prototype)

Each LAT record in Airtable represents one structural unit (article, section, paragraph, etc.) of a law. The fields from `UK.Regulation`:

```
id            — unique article ID within the law
name          — parent law name (foreign key to LRT)
flow          — structural flow marker
type          — record type (article, section, heading, schedule, amendment, etc.)
part          — part number
chapter       — chapter number
heading       — heading text
section       — section/article number
sub_section   — sub-section number
para          — paragraph number
sub_para      — sub-paragraph number
amendment     — amendment annotation
changes       — list of changes
region        — geographic extent (E+W+S+NI per-article)
text          — the actual legal text content
```

Plus counters: `max_amendments`, `max_modifications`, `max_commencements`, `max_extents`, `max_editorials`, `table_counter`, `figure_counter`.

**This is the prototype for Fractalaw's `legislation_text` (LAT) table in LanceDB.** The structural hierarchy (type/part/chapter/section/para) becomes the chunking granularity for embeddings. Each record is one addressable unit of legal text with its position in the document structure.

---

## Progress

- **Task 1 (Schema):** Done. [`docs/SCHEMA.md`](../../docs/SCHEMA.md) (4 tables, 121 columns), [`crates/fractalaw-core/src/schema.rs`](../../crates/fractalaw-core/src/schema.rs) with 16 tests.
- **Task 2 (Data export):** Done. `legislation.parquet` (19,318 rows, 78 cols, 12MB), `law_edges.parquet` (1,035,305 edges, 8 cols, 7.1MB). LAT/amendments parked — see [`docs/SCHEMA-2.0.md`](../../docs/SCHEMA-2.0.md).
- **Task 3 (DuckDB ingestion):** Done. `DuckStore` in `fractalaw-store` with hot path + analytical path queries. 11 tests passing. See [plan](02-17-26-duckdb-ingestion-plan.md).
- **Arrow versioning:** Resolved. duckdb pinned to git main (`rev = a2639608`) for arrow 57. Single arrow version across entire stack. See [02-19-26-arrow-versioning.md](02-19-26-arrow-versioning.md). Revert to crates.io version when duckdb 1.4.5 ships.
- **Task 4 (LanceDB):** [PARKED] — blocked on LAT schema cleanup in sister project.
- **Task 5 (DataFusion):** Done. `FusionStore` in `fractalaw-store/src/fusion.rs` (~470 lines). `DuckTableProvider` delegates scan to DuckDB with projection + limit pushdown. 10 tests passing (49s). Cross-table JOINs verified. Two UDFs: `law_status()`, `edge_type_label()`. Feature-gated behind `duckdb` + `datafusion`. See [plan](02-19-26-plan-wire-up-datafusion.md).
- **Build tooling:** `.cargo/config.toml` with ccache-wrapped brew gcc and `LIBRARY_PATH`. Build skill at `.claude/skills/native-compilation/SKILL.md`. `libduckdb-sys` takes 5-8 min cold, 1-2 min warm.

- **Task 6 (CLI):** Done. `fractalaw-cli` rewritten with clap — 4 commands (`query`, `law`, `graph`, `stats`). Vertical card display in `display.rs` (~310 lines) with 13 schema section groupings and type-aware formatting. `DuckStore` for hot/analytical paths, `FusionStore` for SQL. Binary name `fractalaw`. See [plan](../../.claude/plans/mighty-mapping-star.md).
- **Task 7 (Validation):** Done. All 4 non-parked queries validated against real data. Metadata filter (DataFusion SQL), hot path (HSWA 1974: 177 amended_by in one row read), analytical path (COSHH 2-hop: 1,942 reachable laws), unapplied amendments (185,511 edges). CLI wall time ~4s (Parquet cold-load dominated), query execution sub-ms to ~10ms.

**Committed:** `ad46a7f` — "Implement CLI query interface with stats, law, graph, and query commands"

## Next Step

Phase 1 core tasks (1-3, 5-7) complete. Task 4 (LanceDB) parked on LAT schema cleanup. The architecture is validated: hot path (`List<Struct>` denormalized context), analytical path (recursive edge traversal), and DataFusion SQL all work on 19K legislation / 1M edges. Next steps to consider:
- [#9](https://github.com/fractalaw/fractalaw/issues/9) — Eliminate CLI cold-start latency with persistent DuckDB (persistent `.duckdb` file instead of Parquet cold-load every invocation)
- [#10](https://github.com/fractalaw/fractalaw/issues/10) — Expand dataset beyond UK: multi-jurisdiction Parquet ingestion (EU via EUR-Lex is top candidate)
- [#11](https://github.com/fractalaw/fractalaw/issues/11) — Phase 2: ONNX embeddings, semantic search, and LanceDB integration (blocked on LAT schema cleanup)
- [#12](https://github.com/fractalaw/fractalaw/issues/12) — Build regulation-importer WASM micro-app (Phase 3, requires WIT interfaces + write API)

### GitHub Issues Updated

Existing issues [#1](https://github.com/fractalaw/fractalaw/issues/1), [#7](https://github.com/fractalaw/fractalaw/issues/7), [#8](https://github.com/fractalaw/fractalaw/issues/8) received Phase 1 status update comments noting validated architecture, hot-path performance, and LAT blockers.

---

## Session Closed

**Date**: 2026-02-19

Phase 1 core tasks (1-3, 5-7) complete. Task 4 (LanceDB) parked on LAT schema cleanup. Architecture validated on real data: 19,318 legislation rows, 1,035,305 edges, sub-ms hot path, ~10ms analytical path.

**Commits**:
- `6b0bc12` — Implement DuckDB storage layer and resolve arrow version split
- `0228ef2` — Implement DataFusion unified query layer over DuckDB tables
- `ad46a7f` — Implement CLI query interface with stats, law, graph, and query commands

**Continuation**: [`02-19-26-LAT-schema.md`](02-19-26-LAT-schema.md) — LAT schema revision and baseline data for semantic path.
