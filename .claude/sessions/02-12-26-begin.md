# Session: 2026-02-12 — Phase 1 Begin (v0.4)

## Context

Phase 1 of the fractal-plan roadmap says:

> - Rust workspace with core library crate
> - DuckDB + LanceDB integration with shared Arrow buffers
> - Basic ESH data schema (Arrow schema definitions)
> - Data ingestion pipeline (regulatory documents -> embeddings -> LanceDB)
> - CLI interface for queries

The workspace skeleton is done (6 crates, WIT interfaces, CI, git hooks). Two placeholder Arrow schemas exist in `fractalaw-core` (site_compliance, audit_log). These are generic stubs. The real data model hasn't been designed yet.

## The Gap: Legacy → Fractalaw

The existing system ([sertantai-legal](https://github.com/shotleybuilder/sertantai-legal)) is a Phoenix/Ash app backed by PostgreSQL with ~19K UK legislation records. Its `uk_lrt` table is a wide denormalized table (~80+ columns) with:

**What it has:**
- 6-stage scraping pipeline parsing legislation.gov.uk XML (metadata → extent → enacted_by → amendments → repeal/revoke → taxa/DRRP classification)
- Self-referencing graph relationships: `enacted_by`, `amending`, `amended_by`, `rescinding`, `rescinded_by` — stored as `text[]` arrays of law name keys, plus `linked_*` variants resolved to existing records
- Rich JSONB `*_stats_per_law` structures capturing amendment detail down to individual article targets and affect types
- DRRP holder/taxonomy classification (Duties, Rights, Responsibilities, Powers)
- Geographic extent (E+W+S+NI regions)
- Live status reconciliation from two data sources (changes endpoint vs metadata endpoint, "most severe wins")
- AI via external services (RunPod) — brute force, not local

**What Fractalaw changes:**
- PostgreSQL → DuckDB (analytical) + LanceDB (vector/semantic)
- Row-oriented → Columnar (Arrow)
- Hosted/browser-served → Local-first, edge-deployed
- Cloud AI (RunPod) → Local quantised inference (ONNX, MLC LLM)
- Single server → Hub + edge nodes with sync
- Monolith → WASM micro-apps

This is not a port. It's a re-architecture. The question is: what's the right **data model** for columnar, local-first, graph-aware legislation data?

**Scope: Multi-jurisdiction, multi-language.** The legacy app is UK-only (`uk_lrt`). Fractalaw is designed from the outset for Europe-wide and Australia/New Zealand coverage, with full language support. The schema must not be UK-specific — `type_code` values like `ukpga`, `uksi`, `nisro` are UK legislation.gov.uk concepts. The Fractalaw schema needs a jurisdiction-aware design where UK is the first dataset but the structure accommodates EU directives, French codes, German Verordnungen, Australian Acts, NZ regulations, etc. The ultimate master data source varies per jurisdiction (legislation.gov.uk for UK, EUR-Lex for EU, etc.).

---

## Architecture Decision: Graph-Dense Columnar (v0.2)

**Decision: Hybrid "graph-dense columnar" with three access tiers.**

The guiding constraint is **no pointer chasing** — the CPU should not jump to a different memory address to answer the common case. The fractal-plan's Locality of Reference and Locality of Logic principles map directly to how we partition the data:

### Three Access Tiers

```
┌─────────────────────────────────────────────────────────────┐
│                     HOT PATH (same row)                     │
│              DuckDB: legislation table                       │
│                                                             │
│  Metadata + List<Struct> relationship arrays                │
│  One row = one law + its immediate context                  │
│  Spatial locality: CPU reads one memory block               │
│                                                             │
│  amended_by: [{name, title, year, count, latest_date}, ...] │
│  enacted_by: [{name, title, year}, ...]                     │
│  rescinding: [{name, title, year, count}, ...]              │
├─────────────────────────────────────────────────────────────┤
│                  ANALYTICAL PATH (edge table)               │
│              DuckDB: law_edges table                         │
│                                                             │
│  Flattened edge table for vectorised joins                  │
│  Complex traversals, multi-hop graph queries                │
│  Fan-in/fan-out analytics, amendment network analysis       │
│                                                             │
│  (source_name, target_name, edge_type, article_target,      │
│   affect_type, applied_status, date)                        │
├─────────────────────────────────────────────────────────────┤
│                  SEMANTIC PATH (vector store)               │
│              LanceDB: legislation_text table                 │
│                                                             │
│  Full text, chunked sections, embeddings                    │
│  Semantic search, RAG retrieval                             │
│  AI classifications, extracted obligations                   │
└─────────────────────────────────────────────────────────────┘
```

### Why This Works

**Hot path — `List<Struct>` on the legislation row:**
- The `amended_by` column isn't just `List<Utf8>` (a list of IDs). It's `List<Struct>` carrying **denormalized immediate context**: name, title, year, amendment count, latest date.
- This is **Locality of Logic** — the row contains not just what it references, but enough about the referenced law to answer the common question without a second lookup.
- "Show me Law X and what amended it" = one row read. No join. No pointer chase.
- The legacy `linked_amended_by` + `*_stats_per_law` JSONB are effectively this pattern already — we're making it explicit in Arrow's type system.

**Analytical path — flattened `law_edges` table:**
- For complex queries ("find all laws within 2 hops of the Environmental Protection Act") the `List<Struct>` columns would require `UNNEST` + self-join, which loses the vectorisation benefit.
- A dedicated flattened edge table with `(source, target, type, detail...)` enables DuckDB's morsel-driven parallel joins and columnar scans.
- This is the **search index** — derived from the same data, optimised for a different access pattern.
- The edge table is a materialised view of the relationship data. It can be rebuilt from the legislation table's `List<Struct>` columns.

**Semantic path — LanceDB:**
- Legal text, section-level chunks, embedding vectors.
- DataFusion bridges DuckDB metadata queries with LanceDB semantic search in a single SQL plan.

### Denormalized Immediate Context

The key insight: relationship arrays carry **structs, not just IDs**.

Legacy PostgreSQL:
```
amended_by: ["UK_uksi_2023_381", "UK_uksi_2020_240"]  -- just IDs, need JOIN for title
```

Fractalaw Arrow:
```
amended_by: [
  {name: "UK_uksi_2023_381", title: "The Health and Care...", year: 2023, count: 4, latest_date: 2023-10-15},
  {name: "UK_uksi_2020_240", title: "The Statutory Parental...", year: 2020, count: 1, latest_date: 2020-04-06}
]
```

This is the same principle as the legacy `*_stats_per_law` JSONB — but promoted to a first-class Arrow `List<Struct>` type with named fields, type safety, and columnar encoding. The CPU doesn't need to chase a pointer to another table to display "amended by" in a UI or compliance report.

### Consistency Between Tiers

The hot path and analytical path contain the **same data in different shapes**:
- `legislation.amended_by` (List<Struct>) = the embedded view
- `law_edges WHERE edge_type = 'amended_by'` = the flattened view

On write, both are updated. On read, the query planner (DataFusion) picks the optimal path:
- Single-law lookup → hot path (row read)
- Multi-hop traversal → analytical path (vectorised join)

---

## What Goes Where

| Data | Store | Access Pattern |
|------|-------|---------------|
| Legislation metadata (identity, dates, status, extent, domain, family) | DuckDB: `legislation` (LRT) | Hot path — filter, aggregate, single-law lookup |
| Relationship arrays with immediate context (enacted_by, amending, amended_by, rescinding, rescinded_by) | DuckDB: `legislation` as `List<Struct>` | Hot path — display relationships without join |
| DRRP taxa (duties, rights, responsibilities, powers) | DuckDB: `legislation` as `List<Struct>` | Hot path — compliance matching, categorical filtering |
| Flattened relationship edges with article-level detail | DuckDB: `law_edges` | Analytical path — graph traversal, vectorised joins, amendment network analysis |
| Legislation full text (body, schedules, section chunks) | LanceDB: `legislation_text` (LAT) | Semantic path — embedding source, full-text search |
| Amendment/modification/commencement annotations (F/C/I/E codes) | LanceDB: `amendment_annotations` | Semantic path — annotation detail linked to LAT sections |
| Embedding vectors (per-section) | LanceDB: `legislation_text` | Semantic path — vector similarity, RAG retrieval |
| AI classifications & extracted obligations | LanceDB: `legislation_text` (metadata columns) | Semantic + structured hybrid queries |

---

## The Scraping Pipeline

The existing 6-stage parser in Elixir hits legislation.gov.uk endpoints and builds up each record progressively. This pipeline needs to exist in Fractalaw — it's how data gets in. But now it's a WASM micro-app (`regulation-importer`) that writes through the `fractal:data/mutate` WIT interface.

Not yet though. Phase 1 = get data in from the existing PostgreSQL export. Phase 3 = build the scraper as a micro-app.

---

## Phase 1 Tasks

### 1. Design the Arrow schemas
- [x] **`legislation` schema (LRT)** — 74 columns across 12 sections (identity, classification, dates, territorial extent, document stats, status, function, relationships, DRRP taxa, change logs, timestamps, dropped columns)
- [x] **`law_edges` schema** — 8 columns (source_name, target_name, edge_type, jurisdiction, article_target, affect_type, applied_status, date)
- [x] **`legislation_text` schema (LAT)** — 27 columns (identity/position, structural hierarchy, section types, content, amendment annotation counts, embeddings placeholder, metadata)
- [x] **`amendment_annotations` schema** — 8 columns (identity, annotation codes F/C/I/E, content with affected_sections linkage, metadata)
- [x] Decide which derived columns are stored vs computed at query time (date year/month → computed; amendment counts → stored)
- [x] Document ordering problem identified and resolved (`position` Int32 for document-traversal order, `section_id` as human-readable address)
- [x] Multi-jurisdiction territorial extent: two-layer model (sub-national regions + national)
- [x] `subjects` column: retained with analysis ([md-subjects-analysis](https://github.com/shotleybuilder/sertantai-legal/blob/main/docs/md-subjects-analysis.md))
- [x] Schema documented in [`docs/SCHEMA.md`](../../docs/SCHEMA.md) — 4 tables, struct definitions, column counts, migration path
- [x] Implement schemas in `fractalaw-core/src/schema.rs` — 5 public fns (`legislation_schema` 78 cols, `law_edges_schema` 8 cols, `legislation_text_schema` 27 cols, `amendment_annotations_schema` 8 cols, `audit_log_schema` 10 cols), 3 private helpers (`related_law_struct`, `drrp_entry_struct`, `timestamp_ns_utc`)
- [x] Unit tests: 16 tests — field counts, nullability, `List<Struct>` shape (RelatedLaw 5 fields, DRRPEntry 4 fields), `FixedSizeList<Float32, 384>` embedding, `Timestamp(ns, UTC)`

### 2. Export legacy data from PostgreSQL
- [ ] Write a one-time Elixir `mix task` in sertantai-legal to export `uk_lrt` to Parquet
- [ ] Map LRT columns per SCHEMA.md (set `jurisdiction = "UK"`, `source_authority = "legislation.gov.uk"`, `language = "en"`)
- [ ] Flatten `*_stats_per_law` JSONB into `RelatedLaw` struct arrays
- [ ] Derive `law_edges` Parquet from relationship columns + `*_stats_per_law` detail
- [ ] Export LAT content (article-level text with `position` integers in document-traversal order)
- [ ] Export `amendment_annotations` (F/C/I/E codes with text and affected section linkage)
- [ ] Validate record counts (~19K LRT, expected edge count, LAT article count)

### 3. Build DuckDB ingestion in fractalaw-store
- [ ] Implement `fractalaw-store` with DuckDB feature enabled
- [ ] Load Parquet into DuckDB `legislation` table, mapping to Arrow schemas
- [ ] Load flattened edges Parquet into `law_edges` table
- [ ] Verify: `SELECT count(*) FROM legislation` returns ~19K
- [ ] Verify: hot path query — single law with relationship context (no join needed)
- [ ] Verify: analytical path — "all laws within 2 hops of Law X" via edge table

### 4. Build LanceDB text store in fractalaw-store
- [ ] Create LanceDB `legislation_text` table from LAT schema
- [ ] Create LanceDB `amendment_annotations` table
- [ ] Load text content from export (article-level text with position ordering)
- [ ] Load amendment annotations with `affected_sections` linkage to LAT
- [ ] Embedding vector column present in schema but null — populated in later phase when ONNX is integrated
- [ ] Verify: full-text search works on loaded text (LanceDB native FTS)
- [ ] Verify: metadata filtering works (by law_name, section_type, language)
- [ ] Verify: `ORDER BY position` recovers published document order

### 5. Wire up DataFusion as unified query layer
- [ ] Register DuckDB `TableProvider` for both `legislation` and `law_edges`
- [ ] Register LanceDB `TableProvider` for `legislation_text` and `amendment_annotations`
- [ ] Verify: single SQL query can span DuckDB metadata + LanceDB text search
- [ ] Basic UDFs: `law_status()`, `edge_type_label()`

### 6. Build CLI query interface
- [ ] Extend `fractalaw-cli` with commands:
  - `fractalaw query "SELECT ..."` — SQL via DataFusion
  - `fractalaw law <name>` — show legislation record with denormalized relationships (hot path)
  - `fractalaw graph <name>` — amendment/enactment graph traversal (analytical path)
  - `fractalaw text <name>` — show article text in published order with amendment annotations
  - `fractalaw stats` — dataset summary
- [ ] Arrow pretty-print for tabular output

### 7. Validate the architecture with real queries
- [ ] "Show me all statutory instruments from 2024 affecting Scotland that are still in force" (metadata filter)
- [ ] "What laws does the Health and Safety at Work Act 1974 enact?" (hot path: read `enacted_by` List<Struct> from single row)
- [ ] "Show the full amendment network for COSHH Regulations within 2 hops" (analytical path: recursive edge table traversal)
- [ ] "Which laws have amendments not yet applied?" (hot path: filter on `List<Struct>` field, or analytical: edge table WHERE applied_status = 'Not yet')
- [ ] "Show article text for COSHH reg.7 with all amendment annotations" (semantic path: LAT joined with amendment_annotations)
- [ ] "Find regulations semantically similar to 'workplace noise exposure limits'" (semantic path: LanceDB vector search — requires Phase 2 embeddings)
- [ ] Measure: query latency vs PostgreSQL baseline for equivalent queries

---

## Open Questions (Updated)

1. **Export format**: ~~Parquet vs Arrow IPC~~ → **Parquet**. DuckDB reads it natively with `read_parquet()`, it's self-describing, and it compresses well for the one-time transfer. **Resolved.**

2. **Embeddings**: ~~Bootstrap now or defer?~~ → **Schema design is Phase 1, embeddings are a later phase.** The text table (LAT — Legal Article Text) schema needs to be designed now alongside the legislation table. A prototype exists as an Airtable concept with real data. The LanceDB `legislation_text` table structure — law reference, section chunking, text fields, metadata — is Phase 1 work. Embedding vector columns are defined in the schema but populated later when ONNX inference is integrated. **Resolved.**

3. **Graph traversal performance**: ~~Edge table vs arrays~~ → **Both**. `List<Struct>` for the hot path (single-row context), flattened edge table for analytical traversals. Benchmark both on the real ~19K dataset to confirm the crossover point. **Resolved by architecture decision.**

4. **Legislation full text**: The legacy app stores `md_description` and article-level text from taxa parsing, but not the full legislation body text. Phase 1 can seed LanceDB with what exists. A future legislation.gov.uk full-text scrape (or the `regulation-importer` micro-app in Phase 3) extends this.

5. **Incremental updates**: Phase 1 is a static snapshot. The scraper micro-app (Phase 3) handles ongoing updates. Both tiers (legislation row + edge table) must be updated atomically — this shapes the write API in `fractal:data/mutate`.

6. **Edge table derivation**: ~~Source of truth question~~ → **Legislation table is source of truth.** This is the scraping target — the record is built up progressively through parse stages. The edge table is derived/materialised from the `List<Struct>` relationship columns. Rebuilt on import. Ultimately, the master data is the jurisdiction's official source (legislation.gov.uk for UK, EUR-Lex for EU, etc.) — the legislation table is Fractalaw's local copy of that truth. **Resolved.**

7. **Multi-jurisdiction schema generalisation**: The legacy `uk_lrt` schema has UK-specific fields (`type_code` = `ukpga`/`uksi`/`nisro`, `leg_gov_uk_url`, SI codes, etc.). The Fractalaw schema must generalise:
   - `jurisdiction` field (UK, EU, FR, DE, AU, NZ, ...)
   - `source_authority` (legislation.gov.uk, EUR-Lex, LEGIFRANCE, ...)
   - `instrument_type` that abstracts across legal traditions (Act/SI/Regulation/Directive/Verordnung/Code)
   - `language` (en, fr, de, ...) with multi-language title/text support
   - `source_url` replacing `leg_gov_uk_url`
   - UK-specific fields (SI codes, old_style_number) as nullable or in a jurisdiction-specific extension struct
   - Geographic extent generalised beyond E+W+S+NI to any sub-national region

## Legacy Reference: legl Project (Airtable Prototype)

The [legl project](https://github.com/shotleybuilder/legl) is an earlier Elixir prototype that scraped legislation into Airtable. It provides two critical reference points:

### Multi-jurisdiction source URLs

Each country has its own canonical legislation source, URL structure, and article naming conventions:

| Country | Source | URL Example |
|---------|--------|-------------|
| UK | legislation.gov.uk | `https://www.legislation.gov.uk/uksi/2024/123` |
| DE | retsinformation.dk (note: module says DK but is DE) | `https://www.retsinformation.dk` |
| NO | lovdata.no | `https://lovdata.no/dokument/SF/forskrift/` |
| FIN | finlex.fi | `https://www.finlex.fi` |
| AUT | finlex.fi (Austrian variant) | — |
| SWE | rkrattsbaser.gov.se | `http://rkrattsbaser.gov.se/` |
| TUR | mevzuat.gov.tr | `https://www.mevzuat.gov.tr` |
| RUS | pravo.gov.ru | `https://www.pravo.gov.ru` |
| DK | retsinformation.dk | `https://www.retsinformation.dk` |

**Key observation:** Every jurisdiction has different article-level terminology. The `AirtableSchema` type defines this per-country:
- UK: part → chapter → section → article → paragraph → sub-paragraph, with schedules
- DE/DK: part → chapter → section → sub_section → article → para → sub
- TUR: kisim → bölüm → madde → alt-makale, with ek (annex) and geçici-madde (amendment)
- RUS: chast → razdel → glava → stat'ya → abzats → podpunkt
- NO: kapittel → paragraf → ledd

The Fractalaw LAT schema must accommodate this — **`section_type` needs to be a flexible enum, not hardcoded to UK terminology**. The hierarchy depth varies (UK Acts have sections, UK SIs have articles/regulations).

### LAT Record Shape (from legl prototype)

Each LAT record in Airtable represents one structural unit (article, section, paragraph, etc.) of a law. The fields from `UK.Regulation`:

```
id            — unique article ID within the law
name          — parent law name (foreign key to LRT)
flow          — structural flow marker
type          — record type (article, section, heading, schedule, amendment, etc.)
part          — part number
chapter       — chapter number
heading       — heading text
section       — section/article number
sub_section   — sub-section number
para          — paragraph number
sub_para      — sub-paragraph number
amendment     — amendment annotation
changes       — list of changes
region        — geographic extent (E+W+S+NI per-article)
text          — the actual legal text content
```

Plus counters: `max_amendments`, `max_modifications`, `max_commencements`, `max_extents`, `max_editorials`, `table_counter`, `figure_counter`.

**This is the prototype for Fractalaw's `legislation_text` (LAT) table in LanceDB.** The structural hierarchy (type/part/chapter/section/para) becomes the chunking granularity for embeddings. Each record is one addressable unit of legal text with its position in the document structure.

---

## Next Step

**Task 1 is complete.** Schema designed in [`docs/SCHEMA.md`](../../docs/SCHEMA.md) (4 tables, 121 columns) and implemented in [`crates/fractalaw-core/src/schema.rs`](../../crates/fractalaw-core/src/schema.rs) with 16 tests. All pre-commit checks pass (fmt, check, clippy). Placeholder `site_compliance_schema` removed; `audit_log_schema` retained and refactored to use shared `timestamp_ns_utc()` helper.

Next: Task 2 (legacy data export from PostgreSQL to Parquet).
